{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-22 00:08:41,301] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "# on policy agent : constantly updates how it behaves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_state = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(n_state)\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return of 0.0002\n"
     ]
    }
   ],
   "source": [
    "# agent is not very depentable because \n",
    "# it is more depentdant on how luckily agent choose randome correct rewards \n",
    "# rather than depending upon learning actual policy\n",
    "# so need to choose more appropriate epsilon greedy policy\n",
    "\n",
    "epsilon = 1 # using epsilon greedy strategy \n",
    "gamma = 0.95\n",
    "\n",
    "episodes = 5000\n",
    "Q = np.zeros([n_state, n_actions])\n",
    "rewardTracker = [] \n",
    "\n",
    "alpha = 0.618 #Learning rate\n",
    "G=0\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0, 0\n",
    "    state = env.reset()\n",
    "    firstState = state\n",
    "    \n",
    "    while done != True:\n",
    "        \n",
    "        if np.random.rand() > epsilon: # using epsilon greedy strategy for explore exploit dilemma\n",
    "            action = np.argmax(Q[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            epsilon -= 10**-3\n",
    "   \n",
    "        state2, reward, done, info = env.step(action)\n",
    "        \n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.argmax(Q[state2]- Q[state, action]))        \n",
    "        G += reward\n",
    "        state = state2\n",
    "\n",
    "    rewardTracker.append(G)\n",
    "\n",
    "if (sum(rewardTracker[episode-100:episode])/100.0) > 0.78:\n",
    "    print('--------------------------------------------')\n",
    "    print('solved after {} episodes with avg return of {}'.format(episodeNum-100, sum(rewardTracker[episodeNum])))\n",
    "\n",
    "print(\"Average return of {}\".format(sum(rewardTracker)/len(rewardTracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best way to solve above problme of dependability \n",
    "# is by adding random decaying noise to argmax policy\n",
    "# each time agent takes an action  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_greedy (eps, Q, state, episode):\n",
    "    \n",
    "    if np.random.rand() > eps: # using epsilon greedy strategy for explore exploit dilemma\n",
    "        action = np.argmax(Q[state] + np.random.randn(1,n_actions)/(episode/4))\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        eps -= 10**-5\n",
    "            \n",
    "    return action, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_Q(alpha, gamma, eps, numTrainingEpisodes, numTrainingSteps):\n",
    "\n",
    "    global Q_star\n",
    "    Q = np.zeros([n_state, n_actions])\n",
    "    rewardTracker = [] \n",
    "\n",
    "    for episode in range(1,numTrainingEpisodes+1):\n",
    "        G=0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        firstState = state\n",
    "\n",
    "        for step in range(1,numTrainingSteps+1):\n",
    "            action, eps = e_greedy(eps, Q, state, episode)\n",
    "            \n",
    "            state2, reward, done, info = env.step(action)\n",
    "\n",
    "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.argmax(Q[state2]- Q[state, action]))        \n",
    "            G += reward\n",
    "            state = state2\n",
    "\n",
    "        rewardTracker.append(G)\n",
    "\n",
    "        if episode%(numTrainingEpisodes*.10) == 0 and episode != 0:\n",
    "            print('Alpha {}, Gamma {}, Epsilon{:04.3f} Episode{} of {}'.format(alpha, gamma, eps, episode, numTrainingEpisodes))\n",
    "            print(\"Average total return: {}\".format(sum(rewardTracker)/episode))\n",
    "            print('####-------------------------------------------')\n",
    "            \n",
    "        if (sum(rewardTracker[episode-100:episode])/100.0) > 0.78:\n",
    "            print('--------------------------------------------')\n",
    "            print('solved after {} episodes with avg return of {}'.format(episodeNum-100, sum(rewardTracker[episode-100:episode])))\n",
    "            Q_star = Q\n",
    "            break\n",
    "        \n",
    "    Q_star = Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.5, Gamma 0.99, Epsilon0.005 Episode1000 of 10000\n",
      "Average total return: 0.064\n",
      "####-------------------------------------------\n",
      "Alpha 0.5, Gamma 0.99, Epsilon0.000 Episode2000 of 10000\n",
      "Average total return: 0.062\n",
      "####-------------------------------------------\n",
      "Alpha 0.5, Gamma 0.99, Epsilon0.000 Episode3000 of 10000\n",
      "Average total return: 0.063\n",
      "####-------------------------------------------\n",
      "Alpha 0.5, Gamma 0.99, Epsilon0.000 Episode4000 of 10000\n",
      "Average total return: 0.0625\n",
      "####-------------------------------------------\n",
      "Alpha 0.5, Gamma 0.99, Epsilon0.000 Episode5000 of 10000\n",
      "Average total return: 0.0632\n",
      "####-------------------------------------------\n",
      "Alpha 0.5, Gamma 0.99, Epsilon0.000 Episode6000 of 10000\n",
      "Average total return: 0.06266666666666666\n",
      "####-------------------------------------------\n",
      "Alpha 0.5, Gamma 0.99, Epsilon0.000 Episode7000 of 10000\n",
      "Average total return: 0.064\n",
      "####-------------------------------------------\n",
      "Alpha 0.5, Gamma 0.99, Epsilon0.000 Episode8000 of 10000\n",
      "Average total return: 0.062125\n",
      "####-------------------------------------------\n",
      "Alpha 0.5, Gamma 0.99, Epsilon0.000 Episode9000 of 10000\n",
      "Average total return: 0.06177777777777778\n",
      "####-------------------------------------------\n",
      "Alpha 0.5, Gamma 0.99, Epsilon0.000 Episode10000 of 10000\n",
      "Average total return: 0.0626\n",
      "####-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "learn_Q(0.5, 0.99, 0.1, 10000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (Q, numTrainingEpisodes, numTrainingSteps, render):\n",
    "    rewardTracker = []\n",
    "    \n",
    "    for episode in range(1,numTrainingEpisodes+1):\n",
    "        G=0\n",
    "        state = env.reset()\n",
    "  \n",
    "        for step in range(1,numTrainingSteps+1):\n",
    "            action = np.argmax(Q[state])\n",
    "            \n",
    "            state2, reward, done, info = env.step(action)\n",
    "            state = state2\n",
    "            G += reward\n",
    "\n",
    "            if render == True:\n",
    "                env.render()\n",
    "            if done == True:\n",
    "                break\n",
    "                \n",
    "        rewardTracker.append(G)\n",
    "        \n",
    "        if episode%(numTrainingEpisodes*.10) == 0 and episode != 0:\n",
    "            print(\"Average total return after {} episodes: {:4.3f}\".format(episode, sum(rewardTracker)/episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluate(Q_star, 1, 1000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
